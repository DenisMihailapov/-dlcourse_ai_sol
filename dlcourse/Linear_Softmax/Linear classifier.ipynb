{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Linear classifier.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"p9BMwi2YgECl"},"source":["# Задание 1.2 - Линейный классификатор (Linear classifier)\n","\n","В этом задании мы реализуем другую модель машинного обучения - линейный классификатор. Линейный классификатор подбирает для каждого класса веса, на которые нужно умножить значение каждого признака и потом сложить вместе.\n","Тот класс, у которого эта сумма больше, и является предсказанием модели.\n","\n","В этом задании вы:\n","- потренируетесь считать градиенты различных многомерных функций\n","- реализуете подсчет градиентов через линейную модель и функцию потерь softmax\n","- реализуете процесс тренировки линейного классификатора\n","- подберете параметры тренировки на практике\n","\n","На всякий случай, еще раз ссылка на туториал по numpy:  \n","http://cs231n.github.io/python-numpy-tutorial/"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fOhMRKihgoXk","executionInfo":{"status":"ok","timestamp":1633836416846,"user_tz":-420,"elapsed":23787,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"7bc8264d-de29-49fc-8160-4a42fa2dfdc4"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"1Fip00s_goXk","executionInfo":{"status":"ok","timestamp":1633836416847,"user_tz":-420,"elapsed":11,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}}},"source":["path = '/content/drive/MyDrive/deep learning/Linear_sol'\n","import sys; sys.path.append(path)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"y1NXiYDjgECp","executionInfo":{"status":"ok","timestamp":1633836416847,"user_tz":-420,"elapsed":9,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","%matplotlib inline\n","\n","%load_ext autoreload\n","%autoreload 2"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiMgGmIGgECq","executionInfo":{"status":"ok","timestamp":1633836622241,"user_tz":-420,"elapsed":310,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}}},"source":["from dataset        import load_svhn, random_split_train_val\n","from gradient_check import check_gradient\n","from metrics        import multiclass_accuracy \n","import linear_classifer"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oTb2ktq-gECr"},"source":["# Как всегда, первым делом загружаем данные\n","\n","Мы будем использовать все тот же SVHN."]},{"cell_type":"code","metadata":{"id":"w-BdZFnygECr","executionInfo":{"status":"ok","timestamp":1633836634904,"user_tz":-420,"elapsed":9713,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}}},"source":["def prepare_for_linear_classifier(train_X, test_X):\n","    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n","    test_flat  = test_X .reshape(test_X.shape[0],  -1).astype(np.float) / 255.0\n","    \n","    # Subtract mean\n","    mean_image = np.mean(train_flat, axis = 0)\n","    train_flat -= mean_image\n","    test_flat  -= mean_image\n","    \n","    # Add another channel with ones as a bias term\n","    train_flat_with_ones = np.hstack([train_flat, np.ones((train_X.shape[0], 1))])\n","    test_flat_with_ones = np.hstack([test_flat, np.ones((test_X.shape[0], 1))])    \n","    return train_flat_with_ones, test_flat_with_ones\n","    \n","train_X, train_y, test_X, test_y = load_svhn(path+\"/data\", max_train=10000, max_test=1000)    \n","train_X, test_X = prepare_for_linear_classifier(train_X, test_X)\n","# Split train into train and val\n","train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ELH_cpv6gECs"},"source":["# Играемся с градиентами!\n","\n","В этом курсе мы будем писать много функций, которые вычисляют градиенты аналитическим методом.\n","\n","Необходимым инструментом во время реализации кода, вычисляющего градиенты, является функция его проверки. Эта функция вычисляет градиент численным методом и сверяет результат с градиентом, вычисленным аналитическим методом.\n","\n","Мы начнем с того, чтобы реализовать вычисление численного градиента (numeric gradient) в этой функции.\n","Вычислите градиент с помощью численной производной для каждой координаты. Для вычисления производной используйте так называемую two-point formula (https://en.wikipedia.org/wiki/Numerical_differentiation):\n","\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/22fc2c0a66c63560a349604f8b6b39221566236d)\n"]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"uzIT15MvgECs","executionInfo":{"status":"ok","timestamp":1633836634904,"user_tz":-420,"elapsed":7,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"529bff2f-4355-49df-aa4e-91072e285e30"},"source":["# TODO: Implement gradient check function in gradient_check.py\n","def sqr(x):\n","    return x*x, 2*x\n","\n","check_gradient(sqr, np.array([3.0]))\n","\n","def array_sum(x):\n","    assert x.shape == (2,), x.shape\n","    return np.sum(x), np.ones_like(x)\n","\n","check_gradient(array_sum, np.array([3.0, 2.0]))\n","\n","def array_2d_sum(x):\n","    assert x.shape == (2,2)\n","    return np.sum(x), np.ones_like(x)\n","\n","check_gradient(array_2d_sum, np.array([[3.0, 2.0], [1.0, 0.0]]))"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n","Gradient check passed!\n","Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"thYZJFQ9gECu"},"source":["Теперь реализуем функцию softmax, которая получает на вход оценки для каждого класса и преобразует их в вероятности от 0 до 1:\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3)\n","\n","**Важно:** Практический аспект вычисления этой функции заключается в том, что в ней учавствует вычисление экспоненты от потенциально очень больших чисел - это может привести к очень большим значениям в числителе и знаменателе за пределами диапазона float.\n","\n","К счастью, у этой проблемы есть простое решение -- перед вычислением softmax вычесть из всех оценок максимальное значение среди всех оценок:\n","```\n","predictions -= np.max(predictions)\n","```\n","(подробнее здесь - http://cs231n.github.io/linear-classify/#softmax, секция `Practical issues: Numeric stability`)\n","\n","\n","\n","\n","$\n","\\frac{e^{f_{y_i}}}{\\sum_j e^{f_j}}\n","= \\frac{Ce^{f_{y_i}}}{C\\sum_j e^{f_j}}\n","= \\frac{e^{f_{y_i} + log C}}{\\sum_j e^{f_j + log C}}, \\\\\n","log C = -max (f_{y_i})\n","$\n","\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V5884Cv6gECu","executionInfo":{"status":"ok","timestamp":1633836635771,"user_tz":-420,"elapsed":4,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"fd5a1964-86bf-47d8-95d9-710c38bc8d67"},"source":["# TODO Implement softmax and cross-entropy for single sample\n","probs = linear_classifer.softmax(np.array([-10, 0, 10]))\n","print(probs)\n","print(probs.sum())\n","\n","# Make sure it works for big numbers too!\n","probs = linear_classifer.softmax(np.array([1000, 0, 0]))\n","assert np.isclose(probs[0], 1.0)\n","print(probs)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[2.06106005e-09 4.53978686e-05 9.99954600e-01]\n","0.9999999999999999\n","[1. 0. 0.]\n"]}]},{"cell_type":"markdown","metadata":{"id":"BddfUkPOgECu"},"source":["Кроме этого, мы реализуем cross-entropy loss, которую мы будем использовать как функцию ошибки (error function).\n","В общем виде cross-entropy определена следующим образом:\n","![image](https://wikimedia.org/api/rest_v1/media/math/render/svg/0cb6da032ab424eefdca0884cd4113fe578f4293)\n","\n","где x - все классы, p(x) - истинная вероятность принадлежности сэмпла классу x, а q(x) - вероятность принадлежности классу x, предсказанная моделью.  \n","В нашем случае сэмпл принадлежит только одному классу, индекс которого передается функции. Для него p(x) равна 1, а для остальных классов - 0. \n","\n","Это позволяет реализовать функцию проще!"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KJT02E_ngECv","executionInfo":{"status":"ok","timestamp":1633836639617,"user_tz":-420,"elapsed":553,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"1b54db4b-6977-4588-a35a-3391b1b80806"},"source":["probs = linear_classifer.softmax(np.array([-5, 0, 5]))\n","linear_classifer.cross_entropy_loss(probs, 1)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["5.006760443547122"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","metadata":{"id":"GhzpOHZWgECv"},"source":["После того как мы реализовали сами функции, мы можем реализовать градиент.\n","\n","Оказывается, что вычисление градиента становится гораздо проще, если объединить эти функции в одну, которая сначала вычисляет вероятности через softmax, а потом использует их для вычисления функции ошибки через cross-entropy loss.\n","\n","Эта функция `softmax_with_cross_entropy` будет возвращает и значение ошибки, и градиент по входным параметрам. Мы проверим корректность реализации с помощью `check_gradient`."]},{"cell_type":"markdown","metadata":{"id":"H5lca6LxgECv"},"source":["**P.S.** Зная производную функции *softmax*, мы можем правильно вычислить производную *cross-entropy loss*\n","![image](https://latex.codecogs.com/gif.latex?%5Cbegin%7Balign*%7D%20%5Cfrac%7B%5Cpartial%20smax%28x_i%29%7D%7B%5Cpartial%20x_i%7D%20%26%3D%5Cfrac%7Bf%27%28x%29g%28x%29-f%28x%29g%27%28x%29%7D%7Bg%28x%29%5E2%7D%20%5C%5C%5C%5C%20%26%3D%20%5Cfrac%7Be%5E%7Bx_i%7D%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%20-%20e%5E%7Bx_i%7D%5Cfrac%7B%5Cpartial%7D%7Bx_i%7D%7B%7D%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%20%7D%7B%28%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%29%5E2%7D%7D%5C%5C%5C%5C%20%26%3D%20%5Cfrac%7Be%5E%7Bx_i%7D%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%20-%20%28e%5E%7Bx_i%7D%29%5E2%7D%7B%28%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%29%5E2%7D%7D%5C%5C%5C%5C%20%26%3D%20%5Cleft%20%28%20%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cfrac%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cright%20%29%20-%20%5Cleft%20%28%20%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cright%20%29%5C%5C%5C%5C%20%26%3D%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cleft%20%281-%20%5Cfrac%7Be%5E%7Bx_i%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7B%7CX%7C%7D%7Be%5E%7Bx_j%7D%7D%7D%5Cright%20%29%5C%5C%5C%5C%20%26%3D%20smax%28x_i%29%281-smax%28x_i%29%29%20%5Cend%7Balign*%7D)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8b3yPBaGgECw","executionInfo":{"status":"ok","timestamp":1633836643922,"user_tz":-420,"elapsed":670,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"804818ef-020f-40eb-8793-c6687b3d9311"},"source":["# TODO Implement combined function or softmax and cross entropy and produces gradient\n","loss, grad = linear_classifer.softmax_with_cross_entropy(np.array([1, 0, 0]), 1)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, 1), np.array([1, 0, 0], np.float))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"OrS6lkQ7gECw"},"source":["В качестве метода тренировки мы будем использовать стохастический градиентный спуск (stochastic gradient descent или SGD), который работает с батчами сэмплов. \n","\n","Поэтому все наши фукнции будут получать не один пример, а батч, то есть входом будет не вектор из `num_classes` оценок, а матрица размерности `batch_size, num_classes`. Индекс примера в батче всегда будет первым измерением.\n","\n","Следующий шаг - переписать наши функции так, чтобы они поддерживали батчи.\n","\n","Финальное значение функции ошибки должно остаться числом, и оно равно среднему значению ошибки среди всех примеров в батче."]},{"cell_type":"code","metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"NrMjSFFsgECw","executionInfo":{"status":"ok","timestamp":1633836648815,"user_tz":-420,"elapsed":579,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"6c7718f5-39c8-44e4-f381-1dbda867a052"},"source":["# TODO Extend combined function so it can receive a 2d array with batch of samples\n","np.random.seed(42)\n","\n","num_classes = 4\n","\n","\n","batch_size  = 1\n","predictions  = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n","target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n","\n","\n","batch_size = 3\n","predictions = np.random.randint(-1, 3, size=(batch_size, num_classes)).astype(np.float)\n","target_index = np.random.randint(0, num_classes, size=batch_size).astype(np.int)\n","check_gradient(lambda x: linear_classifer.softmax_with_cross_entropy(x, target_index), predictions)\n"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n","Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"1whUC5wNgECw"},"source":["### Наконец, реализуем сам линейный классификатор!\n","\n","softmax и cross-entropy получают на вход оценки, которые выдает линейный классификатор.\n","\n","Он делает это очень просто: для каждого класса есть набор весов, на которые надо умножить пиксели картинки и сложить. Получившееся число и является оценкой класса, идущей на вход softmax.\n","\n","Таким образом, линейный классификатор можно представить как умножение вектора с пикселями на матрицу W размера `num_features, num_classes`. Такой подход легко расширяется на случай батча векторов с пикселями X размера `batch_size, num_features`:\n","\n","`predictions = X * W`, где `*` - матричное умножение.\n","\n","Реализуйте функцию подсчета линейного классификатора и градиентов по весам `linear_softmax` в файле `linear_classifer.py`"]},{"cell_type":"code","metadata":{"id":"_SnX89IkgECx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633836652087,"user_tz":-420,"elapsed":7,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"7ba6b813-e19a-4c8a-c936-adab27787bc6"},"source":["# TODO Implement linear_softmax function that uses softmax with cross-entropy for linear classifier\n","batch_size = 2\n","num_classes = 2\n","num_features = 3\n","np.random.seed(42)\n","\n","W = np.random.randint(-1, 3, size=(num_features, num_classes)).astype(np.float)\n","X = np.random.randint(-1, 3, size=(batch_size,  num_features)).astype(np.float)\n","target_index = np.ones(batch_size, dtype=np.int)\n","\n","loss, dW = linear_classifer.linear_softmax(X, W, target_index)\n","check_gradient(lambda w: linear_classifer.linear_softmax(X, w, target_index), W)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"iQnWBTAbgECx"},"source":["### И теперь регуляризация\n","\n","Мы будем использовать L2 regularization для весов как часть общей функции ошибки.\n","\n","Напомним, L2 regularization определяется как\n","\n","l2_reg_loss = regularization_strength * sum<sub>ij</sub> W[i, j]<sup>2</sup>\n","\n","Реализуйте функцию для его вычисления и вычисления соотвествующих градиентов."]},{"cell_type":"code","metadata":{"id":"Bqzc8rBDgECx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633836662395,"user_tz":-420,"elapsed":616,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"0c5a599d-e643-4cfe-c5b6-4ab91b3564d6"},"source":["# TODO Implement l2_regularization function that implements loss for L2 regularization\n","linear_classifer.l2_regularization(W, 0.01)\n","check_gradient(lambda w: linear_classifer.l2_regularization(w, 0.01), W)"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Gradient check passed!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","metadata":{"id":"JwoApboegECy"},"source":["# Тренировка!"]},{"cell_type":"markdown","metadata":{"id":"_d9rZa0YgECy"},"source":["Градиенты в порядке, реализуем процесс тренировки!"]},{"cell_type":"code","metadata":{"scrolled":false,"id":"PzhZAVhQgECy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633837472081,"user_tz":-420,"elapsed":34704,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"4400c782-4fd1-410f-e0f9-3288e92c35be"},"source":["# TODO: Implement LinearSoftmaxClassifier.fit function\n","classifier = linear_classifer.LinearSoftmaxClassifier(learning_rate=1e-3, reg_rate=1e1)\n","loss_history = classifier.fit(train_X, train_y, epochs=2000, batch_size=500)"],"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0, loss: 2.607971\n","Epoch 10, loss: 2.505715\n","Epoch 20, loss: 2.437716\n","Epoch 30, loss: 2.393133\n","Epoch 40, loss: 2.362792\n","Epoch 50, loss: 2.341607\n","Epoch 60, loss: 2.329243\n","Epoch 70, loss: 2.319277\n","Epoch 80, loss: 2.313977\n","Epoch 90, loss: 2.310180\n","Epoch 100, loss: 2.307925\n","Epoch 110, loss: 2.306026\n","Epoch 120, loss: 2.304165\n","Epoch 130, loss: 2.304088\n","Epoch 140, loss: 2.302620\n","Epoch 150, loss: 2.302458\n","Epoch 160, loss: 2.301161\n","Epoch 170, loss: 2.302448\n","Epoch 180, loss: 2.301958\n","Epoch 190, loss: 2.302456\n","Epoch 200, loss: 2.301674\n","Epoch 210, loss: 2.301381\n","Epoch 220, loss: 2.301837\n","Epoch 230, loss: 2.302290\n","Epoch 240, loss: 2.301859\n","Epoch 250, loss: 2.301909\n","Epoch 260, loss: 2.302140\n","Epoch 270, loss: 2.301570\n","Epoch 280, loss: 2.301863\n","Epoch 290, loss: 2.301783\n","Epoch 300, loss: 2.301764\n","Epoch 310, loss: 2.301512\n","Epoch 320, loss: 2.302440\n","Epoch 330, loss: 2.302044\n","Epoch 340, loss: 2.301061\n","Epoch 350, loss: 2.302445\n","Epoch 360, loss: 2.301282\n","Epoch 370, loss: 2.301624\n","Epoch 380, loss: 2.302533\n","Epoch 390, loss: 2.301425\n","Epoch 400, loss: 2.301827\n","Epoch 410, loss: 2.301249\n","Epoch 420, loss: 2.301314\n","Epoch 430, loss: 2.302161\n","Epoch 440, loss: 2.302008\n","Epoch 450, loss: 2.302527\n","Epoch 460, loss: 2.301613\n","Epoch 470, loss: 2.302087\n","Epoch 480, loss: 2.301677\n","Epoch 490, loss: 2.301725\n","Epoch 500, loss: 2.301256\n","Epoch 510, loss: 2.301204\n","Epoch 520, loss: 2.301713\n","Epoch 530, loss: 2.301560\n","Epoch 540, loss: 2.302447\n","Epoch 550, loss: 2.301793\n","Epoch 560, loss: 2.301988\n","Epoch 570, loss: 2.301648\n","Epoch 580, loss: 2.301459\n","Epoch 590, loss: 2.302169\n","Epoch 600, loss: 2.302178\n","Epoch 610, loss: 2.302146\n","Epoch 620, loss: 2.301917\n","Epoch 630, loss: 2.300772\n","Epoch 640, loss: 2.300952\n","Epoch 650, loss: 2.301452\n","Epoch 660, loss: 2.301448\n","Epoch 670, loss: 2.301879\n","Epoch 680, loss: 2.301409\n","Epoch 690, loss: 2.301847\n","Epoch 700, loss: 2.302177\n","Epoch 710, loss: 2.301411\n","Epoch 720, loss: 2.302658\n","Epoch 730, loss: 2.301763\n","Epoch 740, loss: 2.301385\n","Epoch 750, loss: 2.302546\n","Epoch 760, loss: 2.301899\n","Epoch 770, loss: 2.301423\n","Epoch 780, loss: 2.301852\n","Epoch 790, loss: 2.301800\n","Epoch 800, loss: 2.302308\n","Epoch 810, loss: 2.301968\n","Epoch 820, loss: 2.301027\n","Epoch 830, loss: 2.302694\n","Epoch 840, loss: 2.301409\n","Epoch 850, loss: 2.302601\n","Epoch 860, loss: 2.301389\n","Epoch 870, loss: 2.301516\n","Epoch 880, loss: 2.302285\n","Epoch 890, loss: 2.301260\n","Epoch 900, loss: 2.301766\n","Epoch 910, loss: 2.301473\n","Epoch 920, loss: 2.302167\n","Epoch 930, loss: 2.302491\n","Epoch 940, loss: 2.300628\n","Epoch 950, loss: 2.302230\n","Epoch 960, loss: 2.301723\n","Epoch 970, loss: 2.301897\n","Epoch 980, loss: 2.301984\n","Epoch 990, loss: 2.300425\n","Epoch 1000, loss: 2.302094\n","Epoch 1010, loss: 2.300964\n","Epoch 1020, loss: 2.301576\n","Epoch 1030, loss: 2.301118\n","Epoch 1040, loss: 2.301359\n","Epoch 1050, loss: 2.301628\n","Epoch 1060, loss: 2.300697\n","Epoch 1070, loss: 2.301919\n","Epoch 1080, loss: 2.302505\n","Epoch 1090, loss: 2.301947\n","Epoch 1100, loss: 2.302257\n","Epoch 1110, loss: 2.302356\n","Epoch 1120, loss: 2.301396\n","Epoch 1130, loss: 2.301968\n","Epoch 1140, loss: 2.302240\n","Epoch 1150, loss: 2.301347\n","Epoch 1160, loss: 2.301914\n","Epoch 1170, loss: 2.301971\n","Epoch 1180, loss: 2.302551\n","Epoch 1190, loss: 2.302493\n","Epoch 1200, loss: 2.301468\n","Epoch 1210, loss: 2.301980\n","Epoch 1220, loss: 2.301857\n","Epoch 1230, loss: 2.301571\n","Epoch 1240, loss: 2.302404\n","Epoch 1250, loss: 2.301212\n","Epoch 1260, loss: 2.301694\n","Epoch 1270, loss: 2.301535\n","Epoch 1280, loss: 2.302467\n","Epoch 1290, loss: 2.302108\n","Epoch 1300, loss: 2.301860\n","Epoch 1310, loss: 2.301712\n","Epoch 1320, loss: 2.301286\n","Epoch 1330, loss: 2.302081\n","Epoch 1340, loss: 2.302272\n","Epoch 1350, loss: 2.302010\n","Epoch 1360, loss: 2.301631\n","Epoch 1370, loss: 2.301169\n","Epoch 1380, loss: 2.301992\n","Epoch 1390, loss: 2.301623\n","Epoch 1400, loss: 2.301713\n","Epoch 1410, loss: 2.300516\n","Epoch 1420, loss: 2.301586\n","Epoch 1430, loss: 2.301892\n","Epoch 1440, loss: 2.302413\n","Epoch 1450, loss: 2.301988\n","Epoch 1460, loss: 2.302198\n","Epoch 1470, loss: 2.302188\n","Epoch 1480, loss: 2.301922\n","Epoch 1490, loss: 2.301883\n","Epoch 1500, loss: 2.301493\n","Epoch 1510, loss: 2.301790\n","Epoch 1520, loss: 2.302406\n","Epoch 1530, loss: 2.300930\n","Epoch 1540, loss: 2.301131\n","Epoch 1550, loss: 2.302421\n","Epoch 1560, loss: 2.301525\n","Epoch 1570, loss: 2.301963\n","Epoch 1580, loss: 2.302355\n","Epoch 1590, loss: 2.301620\n","Epoch 1600, loss: 2.302121\n","Epoch 1610, loss: 2.301915\n","Epoch 1620, loss: 2.301957\n","Epoch 1630, loss: 2.301833\n","Epoch 1640, loss: 2.301587\n","Epoch 1650, loss: 2.301889\n","Epoch 1660, loss: 2.302017\n","Epoch 1670, loss: 2.301733\n","Epoch 1680, loss: 2.301539\n","Epoch 1690, loss: 2.301542\n","Epoch 1700, loss: 2.300629\n","Epoch 1710, loss: 2.302084\n","Epoch 1720, loss: 2.301991\n","Epoch 1730, loss: 2.301718\n","Epoch 1740, loss: 2.301690\n","Epoch 1750, loss: 2.301392\n","Epoch 1760, loss: 2.301736\n","Epoch 1770, loss: 2.301686\n","Epoch 1780, loss: 2.301638\n","Epoch 1790, loss: 2.301832\n","Epoch 1800, loss: 2.301694\n","Epoch 1810, loss: 2.301235\n","Epoch 1820, loss: 2.302189\n","Epoch 1830, loss: 2.301935\n","Epoch 1840, loss: 2.302044\n","Epoch 1850, loss: 2.301224\n","Epoch 1860, loss: 2.302195\n","Epoch 1870, loss: 2.301341\n","Epoch 1880, loss: 2.301659\n","Epoch 1890, loss: 2.301703\n","Epoch 1900, loss: 2.301922\n","Epoch 1910, loss: 2.301795\n","Epoch 1920, loss: 2.302471\n","Epoch 1930, loss: 2.300850\n","Epoch 1940, loss: 2.300308\n","Epoch 1950, loss: 2.302154\n","Epoch 1960, loss: 2.301890\n","Epoch 1970, loss: 2.300605\n","Epoch 1980, loss: 2.301711\n","Epoch 1990, loss: 2.302342\n"]}]},{"cell_type":"code","metadata":{"id":"r0SGO3bqgECy","colab":{"base_uri":"https://localhost:8080/","height":265},"executionInfo":{"status":"ok","timestamp":1633837482122,"user_tz":-420,"elapsed":815,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"05b3b7e1-5ae2-4dab-c8c6-b4061e92c42b"},"source":["# let's look at the loss history!\n","plt.plot(loss_history);"],"execution_count":25,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"WXyMplLQgECz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633837490013,"user_tz":-420,"elapsed":1065,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"8b65663e-5643-44b6-eb6a-477de0b55bcf"},"source":["# Let's check how it performs on validation set\n","pred = classifier.predict(val_X)\n","accuracy = multiclass_accuracy(pred, val_y)\n","print(\"Accuracy: \", accuracy)\n","\n","# Now, let's train more and see if it performs better\n","classifier.fit(train_X, train_y, epochs=100, batch_size=300)\n","pred = classifier.predict(val_X)\n","accuracy = multiclass_accuracy(pred, val_y)\n","print(\"Accuracy after training for 100 epochs: \", accuracy)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.123\n","Epoch 0, loss: 2.607777\n","Epoch 10, loss: 2.506763\n","Epoch 20, loss: 2.438604\n","Epoch 30, loss: 2.392966\n","Epoch 40, loss: 2.362793\n","Epoch 50, loss: 2.342231\n","Epoch 60, loss: 2.330987\n","Epoch 70, loss: 2.320442\n","Epoch 80, loss: 2.313810\n","Epoch 90, loss: 2.310304\n","Accuracy after training for 100 epochs:  0.123\n"]}]},{"cell_type":"markdown","metadata":{"id":"NYDrjbUwgECz"},"source":["### Как и раньше, используем кросс-валидацию для подбора гиперпараметтов.\n","\n","В этот раз, чтобы тренировка занимала разумное время, мы будем использовать только одно разделение на тренировочные (training) и проверочные (validation) данные.\n","\n","Теперь нам нужно подобрать не один, а два гиперпараметра! Не ограничивайте себя изначальными значениями в коде.  \n","Добейтесь точности более чем **20%** на проверочных данных (validation data)."]},{"cell_type":"code","metadata":{"id":"20uIh8x0gECz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633837542257,"user_tz":-420,"elapsed":43643,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"3ac0fb18-612d-4faa-80cd-afafa2f78f24"},"source":["%%time\n","num_epochs = 200\n","batch_size = 300\n","\n","learning_rates = [1e-2, 1e-3, 1e-4, 1e-5, 1e-6]\n","reg_strengths = [1e-3, 1e-4, 1e-5, 1e-6, 1e-7]\n","\n","best_val_accuracy = 0\n","best_loss_history = None\n","best_classifier = None\n","best_params = {'learning_rate': None,\n","               'reg_strength': None}\n","\n","# TODO use validation set to find the best hyperparameters\n","# hint: for best results, you might need to try more values for learning rate and regularization strength \n","# than provided initially\n","for lr in learning_rates:\n","    for reg in reg_strengths:\n","        classifier = linear_classifer.LinearSoftmaxClassifier()\n","        loss_history = classifier.fit(train_X, train_y, verbose=False,\n","                                      epochs=num_epochs, batch_size=batch_size,\n","                                      learning_rate=lr, reg_rate=reg)\n","        \n","        pred = classifier.predict(val_X)\n","        accuracy = multiclass_accuracy(pred, val_y)\n","        print(\"Accuracy with lr={0}, reg={1}: {2}\".format(lr, reg, accuracy))\n","        \n","        if accuracy > best_val_accuracy:\n","            best_val_accuracy = accuracy\n","            best_loss_history = loss_history\n","            best_classifier = classifier\n","            best_params['learning_rate'] = lr\n","            best_params['reg_strength'] = reg\n","        \n","print()\n","print('best validation accuracy achieved: %f' % best_val_accuracy)"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy with lr=0.01, reg=0.001: 0.091\n","Accuracy with lr=0.01, reg=0.0001: 0.107\n","Accuracy with lr=0.01, reg=1e-05: 0.117\n","Accuracy with lr=0.01, reg=1e-06: 0.113\n","Accuracy with lr=0.01, reg=1e-07: 0.096\n","Accuracy with lr=0.001, reg=0.001: 0.108\n","Accuracy with lr=0.001, reg=0.0001: 0.089\n","Accuracy with lr=0.001, reg=1e-05: 0.099\n","Accuracy with lr=0.001, reg=1e-06: 0.094\n","Accuracy with lr=0.001, reg=1e-07: 0.093\n","Accuracy with lr=0.0001, reg=0.001: 0.132\n","Accuracy with lr=0.0001, reg=0.0001: 0.088\n","Accuracy with lr=0.0001, reg=1e-05: 0.101\n","Accuracy with lr=0.0001, reg=1e-06: 0.107\n","Accuracy with lr=0.0001, reg=1e-07: 0.111\n","Accuracy with lr=1e-05, reg=0.001: 0.107\n","Accuracy with lr=1e-05, reg=0.0001: 0.101\n","Accuracy with lr=1e-05, reg=1e-05: 0.087\n","Accuracy with lr=1e-05, reg=1e-06: 0.09\n","Accuracy with lr=1e-05, reg=1e-07: 0.148\n","Accuracy with lr=1e-06, reg=0.001: 0.091\n","Accuracy with lr=1e-06, reg=0.0001: 0.103\n","Accuracy with lr=1e-06, reg=1e-05: 0.132\n","Accuracy with lr=1e-06, reg=1e-06: 0.101\n","Accuracy with lr=1e-06, reg=1e-07: 0.076\n","\n","best validation accuracy achieved: 0.148000\n","CPU times: user 1min 8s, sys: 15.8 s, total: 1min 24s\n","Wall time: 43.3 s\n"]}]},{"cell_type":"markdown","metadata":{"id":"2Ftgg4BCgECz"},"source":["# Какой же точности мы добились на тестовых данных?"]},{"cell_type":"code","metadata":{"id":"YdQS43kPgEC0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1633837564111,"user_tz":-420,"elapsed":282,"user":{"displayName":"Денис Иванович Михайлапов","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07025369822590947805"}},"outputId":"cecb0f84-8881-4c3e-e7a9-3a97b70d9445"},"source":["test_pred = best_classifier.predict(test_X)\n","test_accuracy = multiclass_accuracy(test_pred, test_y)\n","print('Linear softmax classifier test set accuracy: %f' % (test_accuracy, ))"],"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear softmax classifier test set accuracy: 0.116000\n"]}]}]}